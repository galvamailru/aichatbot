=== RESULT FOR EXECUTION-SPEC ===
Задача: Сервер с AI-агентом в виде чата, встраиваемого в HTML через iframe. Реализация на Python в Docker с быстрым запуском через docker-compose. LLM — DeepSeek с возможностью смены URL. Диалоги разделены по USER_ID и хранятся в PostgreSQL.
Выбранный подход: FastAPI-сервер с REST API для отправки сообщений и Server-Sent Events (SSE) для стриминга ответа LLM. Конфигурация — только из .env. Промпт LLM загружается из отдельного файла. Сохранение диалогов в PostgreSQL по user_id после завершения стрима (или по мере поступления — на усмотрение реализации). Клиент в iframe: форма отправки + приём SSE и пошаговый вывод ответа.
Ключевые инварианты:
Один запрос пользователя соответствует одному вызову LLM и одному SSE-потоку.
Все переменные окружения задаются только в .env.
Промпт LLM читается только из файла, путь/имя — из конфигурации.
Диалоги изолированы по USER_ID; в БД хранятся сообщения с привязкой к user_id (и при необходимости к диалогу/сессии).
Жёсткие ограничения:
Язык реализации — Python. Стек — FastAPI, PostgreSQL, Docker/docker-compose.
Запрещено хранить секреты и конфигурацию вне .env.
Запрещено хардкодить промпт LLM в коде; только чтение из файла.
URL LLM-сервера должен настраиваться через переменную окружения (или .env).
Границы ответственности:
Сервер обеспечивает API (REST + SSE), вызов LLM DeepSeek, персистенс в PostgreSQL и раздачу статики/страницы для iframe (или только API, если iframe-страница вынесена на сторону клиента — уточнить в ТЗ). Не входит в границы: реализация внешнего фронтенда вне iframe, администрирование БД, настройка инфраструктуры за пределами docker-compose.
